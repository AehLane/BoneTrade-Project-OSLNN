{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human Remains OSLNN - Triplet Loss (WIP).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AehLane/BoneTrade-Project-OSLNN/blob/master/Human_Remains_OSLNN_Triplet_Loss_(WIP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWeTgO_e6Y9e",
        "colab_type": "text"
      },
      "source": [
        "# One Shot Learning with Siamese Networks\n",
        "\n",
        "Code adapated from Harshvardhan Gupta, 2017 [One Shot Learning With Siamese Networks](https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/blob/master/Siamese-networks-medium.ipynb) and discussed in \n",
        "Graham and Huffer, Can One-Shot Learning Identify Origins of Human Remains Shown on Social Media? \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We're assuming you have loaded this notebook with Google Colab and that you are signed into a Google account. You run each block of code in sequential order the first time through. If after having trained a network you wish to resume testing at a later date, we show you how to save your model and reload it, at the appropriate blocks.\n",
        "\n",
        "## Hardware Acceleration\n",
        "\n",
        "Under 'Edit' -> 'Notebook Settings' make sure to select 'GPU' for Hardware Acceleration. The next block of code double-checks to make sure you've got GPU selected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjVyWDJAF4-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrQFGF1T8wgI",
        "colab_type": "text"
      },
      "source": [
        "## Load your data\n",
        "\n",
        "There are a variety of ways to get your data into this notebook. The easiest are to upload directly, or to connect your Google Drive account. If you connect your Google Drive account, it will appear as a folder within the tray. You can right-click on a folder within your drive, copy the path, and then paste that into your code. Note that you'll have to delete the leading `/content/` from the path. \n",
        "\n",
        "**We are assuming that you have zipped a folder called `data` with two subfolders `testing` and `training`, and within each of those folders, one folder per class of _item_ you wish to train on/test.**\n",
        "\n",
        "**To upload** open the tray at left by clicking on the `>` button. Select 'Files' and then 'Upload'. You can then select a zip file from your machine. The tray does not refresh automatically, so you'll need to hit 'refresh' periodically.\n",
        "\n",
        "Then, unzip the data by running the next block. If you're using Google drive, skip to the next block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byy27-Z_EHgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If I'm uploading\n",
        "!unzip data.zip -d data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp4KELeH9wcp",
        "colab_type": "text"
      },
      "source": [
        "**To mount your Google drive** run the next block of code. The results block will display a URL. Click on this, and a new window will open, confirming that you want to connect your drive. Once you've confirmed, an authorization code will be displayed. Copy this code, and paste into the results block. If all goes well, the results block will shortly display the text, 'Mounted at /content/drive'. Refresh the files pane in the tray at left, and you will see it there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne9mUU_TQFLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ft2c2VM-Zx7",
        "colab_type": "text"
      },
      "source": [
        "In the two blocks below, we show you how to copy a file from your drive to this space, how to make a new directory (`mkdir`) and how to unzip a folder into that new directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0rfcjBnQbVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you have data already on google drive\n",
        "!cp \"drive/My Drive/one-shot-test/data_copy_with_equal_augs.zip\" data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiy0t5z3J1Is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload data from local machine then:\n",
        "# !mkdir data && unzip data.zip -d data/\n",
        "!unzip data.zip \n",
        "\n",
        "# Or copy it over from google drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5oU-S9i-mkz",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Imports\n",
        "\n",
        "We now need to import some libraries that we will use to build the neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l92t8_-FOeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import PIL.ImageOps    \n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "#!pip install tensorflow-addons\n",
        "#import tensorflow_addons as tfa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0yyYaOq-031",
        "colab_type": "text"
      },
      "source": [
        "## Helper functions\n",
        "\n",
        "Here we create two helper functions, `imshow` and `show_plot`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE-TfRPnGGxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img,text=None, text2=None,should_save=False):\n",
        "    npimg = img.numpy()\n",
        "    plt.axis(\"off\")\n",
        "    if text:\n",
        "        plt.text(10, 10, text, style='italic',fontweight='bold',\n",
        "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
        "    if text2:\n",
        "        plt.text(120, 10, text2, style='italic',fontweight='bold',\n",
        "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()    \n",
        "\n",
        "def show_plot(iteration,loss):\n",
        "    plt.plot(iteration,loss)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OMg3RNu-QsV",
        "colab_type": "text"
      },
      "source": [
        "## Defining Dataloader workers' init()\n",
        "\n",
        "We specify an init function, \"__worker_init_fn__\" to split the task of generating anchor-unknown pairs among a group of workers. This function is provided as an argument for __DataLoaders__, giving the dataloader an init method for its workers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dv5acBUGTpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def worker_init_fn(worker_id):\n",
        "    worker_info = torch.utils.data.get_worker_info()\n",
        "    dataset = worker_info.dataset\n",
        "\n",
        "    dataset.worker_unique_id = worker_info.id\n",
        "    dataset.number_of_workers = worker_info.num_workers\n",
        "    dataset.anchor_iter_index = torch.tensor(worker_info.id)\n",
        "    dataset.unknown_iter_index = torch.tensor(worker_info.id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hh2Y_l8_bpY",
        "colab_type": "text"
      },
      "source": [
        "## Configuration class\n",
        "\n",
        "We now create a class to configure some variables we will be reusing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml-XlyHEGKB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config():\n",
        "    training_dir = \"/content/data_copy_with_equal_augs/data/training\"\n",
        "    testing_dir = \"/content/data_copy_with_equal_augs/data/testing\"\n",
        "    train_batch_size = 64\n",
        "    train_number_epochs = 100\n",
        "    num_generator_workers = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC_2Oc5L_8V4",
        "colab_type": "text"
      },
      "source": [
        "## Custom dataset class\n",
        "\n",
        "Under a superclass of \"__SiameseNetworkDataset__\", two subclasses exist for different purposes; training and testing.\n",
        "\n",
        "For \"__PairwiseTrainingSiameseNetworkDataset__\", we set up the code to generates a pair of images from our training dataset, 0 for geniune pair and 1 for imposter pair. It goes through the training directory, pairing images and assuming that images within a subfolder are similar and images from different subfolders are different. This is also where we attach filenames to the images so that when we are testing later we know which pairs of images we're dealing with. This addition to the original code, the class implementation now residing under \"__PairwiseTrainingSiameseNetworkDataset__\", is courtesy of Tim Sherratt with modifications by Alex Lane.\n",
        "\n",
        "__Note:__ If the training dataset subdirectories do not have the same number of images in each subdirectory, \"__PairwiseTrainingSiameseNetworkDataset__\" will select pairs with more bias. Over a large number of pairs, more pairs containing images from subdirectories with less images will be generated. It is advised that the subdirectories have a uniform number of images within to ensure random selection.\n",
        "\n",
        "For \"__PairwiseTestingSiameseNetworkDataset__\", \"__getitem__\" returns a valid anchor-unknown pair of images. Since the data is kept as a generator due to possibly working with large sets of data, the runtime should be __O(n^2)__ due to iteratively comparing each element of the dataset to each potential pair member in the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3HDi7pUvNlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "import imageio\n",
        "\n",
        "def generate_positive(anchor_img_path):\n",
        "    random.seed()\n",
        "    t = int(random.randint(0, 100000))\n",
        "    #print(t)\n",
        "    ia.seed(t)\n",
        "\n",
        "    #print(\"before read\")\n",
        "    #print(anchor_img_path)\n",
        "    img = imageio.imread(anchor_img_path) #read you image\n",
        "    #print(\"after read\")\n",
        "    images = np.array(\n",
        "        [img for _ in range(1)], dtype=np.uint8)  # 32 means creat 32 enhanced images using following methods.\n",
        "\n",
        "    seq = iaa.Sequential(\n",
        "        [\n",
        "            iaa.Fliplr(0.5),  \n",
        "            iaa.Crop(percent=(0, 0.1)),            \n",
        "            iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.5))),        \n",
        "            iaa.ContrastNormalization((0.75, 1.5)),         \n",
        "            iaa.AdditiveGaussianNoise(\n",
        "                loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5),    \n",
        "            iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
        "            iaa.Affine(\n",
        "                scale={\n",
        "                    \"x\": (0.8, 1.2),\n",
        "                    \"y\": (0.8, 1.2)\n",
        "                },\n",
        "                translate_percent={\n",
        "                    \"x\": (-0.2, 0.2),\n",
        "                    \"y\": (-0.2, 0.2)\n",
        "                },\n",
        "                rotate=(-25, 25),\n",
        "                shear=(-8, 8))\n",
        "        ],\n",
        "        random_order=True)  # apply augmenters in random order\n",
        "\n",
        "    images_aug = seq.augment_images(images)\n",
        "    pos_img = Image.fromarray(images_aug[0])\n",
        "\n",
        "    #imshow(torchvision.utils.make_grid(pos_img))\n",
        "\n",
        "    return pos_img\n",
        "    #imageio.imwrite(anchor_img_path+'_generated_positive.png', images_aug[0])\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C49BrRm0GdTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetworkDataset(Dataset):\n",
        "    \n",
        "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
        "        self.imageFolderDataset = imageFolderDataset    \n",
        "        self.transform = transform\n",
        "        self.should_invert = should_invert\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.imageFolderDataset.imgs)\n",
        "\n",
        "\n",
        "class PairwiseTrainingSiameseNetworkDataset(SiameseNetworkDataset):\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
        "        # We need to make sure approx 50% of images are in the same class\n",
        "        should_get_same_class = bool(random.getrandbits(1)) \n",
        "\n",
        "        num_pos_images = (len(self.imageFolderDataset.imgs)\n",
        "            /len(self.imageFolderDataset.class_to_idx))\n",
        "        if should_get_same_class:\n",
        "            while True:\n",
        "            #    # Keep looping till the same class image is found\n",
        "            #   img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
        "            #   if img0_tuple[1]==img1_tuple[1]:\n",
        "            #       break\n",
        "                if (img0_tuple != self.imageFolderDataset.imgs[random.randrange((img0_tuple[1]*num_pos_images),((img0_tuple[1]+1)*num_pos_images))]):\n",
        "                    img1_tuple = self.imageFolderDataset.imgs[random.randrange((img0_tuple[1]*num_pos_images),((img0_tuple[1]+1)*num_pos_images))]\n",
        "                    break;\n",
        "        else:\n",
        "            #while True:\n",
        "            #    # Keep looping till a different class image is found\n",
        "            #    img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
        "            #    if img0_tuple[1] !=img1_tuple[1]:\n",
        "            #        break\n",
        "            size_first_neg_range = img0_tuple[1]*num_pos_images\n",
        "            size_second_neg_range = len(self.imageFolderDataset.imgs) - ((img0_tuple[1]+1)*num_pos_images)\n",
        "\n",
        "            total_random_neg_indices = size_first_neg_range+size_second_neg_range\n",
        "            neg_index_chosen = int(total_random_neg_indices * random.random())\n",
        "            if(neg_index_chosen < size_first_neg_range):\n",
        "                img1_tuple = self.imageFolderDataset.imgs[int(neg_index_chosen)]\n",
        "            else:\n",
        "                img1_tuple = self.imageFolderDataset.imgs[int(neg_index_chosen+num_pos_images)]\n",
        "\n",
        "\n",
        "        img0 = Image.open(img0_tuple[0])\n",
        "        img1 = Image.open(img1_tuple[0])\n",
        "        img0 = img0.convert(\"L\")\n",
        "        img1 = img1.convert(\"L\")\n",
        "            \n",
        "        if self.should_invert:\n",
        "            img0 = PIL.ImageOps.invert(img0)\n",
        "            img1 = PIL.ImageOps.invert(img1)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img0 = self.transform(img0)\n",
        "            img1 = self.transform(img1)\n",
        "        \n",
        "        # return img0, img1 , torch.from_numpy(\n",
        "            # np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
        "\t\t# Courtesy of Tim Sherrat\n",
        "        # The new return line\n",
        "\t\t# Note the addition of img0_tuple[0], img1_tuple[0] which contain the \n",
        "        # paths\n",
        "        #print(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
        "        return (img0,\n",
        "                img1,\n",
        "                torch.from_numpy(\n",
        "                    np.array(\n",
        "                        [int(img1_tuple[1]!=img0_tuple[1])],\n",
        "                        dtype=np.float32)),\n",
        "                img0_tuple[0],\n",
        "                img1_tuple[0])\n",
        "\n",
        "\n",
        "class TripletTrainingSiameseNetworkDataset(SiameseNetworkDataset):\n",
        "\n",
        "    # Returns an anchor image, a positive image (in the same class), and a \n",
        "    # negative image (in a different class)\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "\n",
        "\n",
        "        # First, we select a random image to be our anchor\n",
        "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
        "        #print(img0_tuple)\n",
        "\n",
        "        # Secondly, we select a random image in the same class as our positive\n",
        "        num_pos_images = (len(self.imageFolderDataset.imgs)\n",
        "            /len(self.imageFolderDataset.class_to_idx))\n",
        "        img1_tuple = self.imageFolderDataset.imgs[random.randrange((img0_tuple[1]*num_pos_images),((img0_tuple[1]+1)*num_pos_images))]\n",
        "\n",
        "        #Thirdly, we select a random image in a different class as the negative\n",
        "        size_first_neg_range = img0_tuple[1]*num_pos_images\n",
        "        size_second_neg_range = len(self.imageFolderDataset.imgs) - ((img0_tuple[1]+1)*num_pos_images)\n",
        "\n",
        "        total_random_neg_indices = size_first_neg_range+size_second_neg_range\n",
        "        neg_index_chosen = int(total_random_neg_indices * random.random())\n",
        "        if(neg_index_chosen < size_first_neg_range):\n",
        "            img2_tuple = self.imageFolderDataset.imgs[int(neg_index_chosen)]\n",
        "        else:\n",
        "            img2_tuple = self.imageFolderDataset.imgs[int(neg_index_chosen+num_pos_images)]\n",
        "\n",
        "        img0 = Image.open(img0_tuple[0])\n",
        "        img1 = Image.open(img1_tuple[0])\n",
        "        img2 = Image.open(img2_tuple[0])\n",
        "        img0 = img0.convert(\"L\")\n",
        "        img1 = img1.convert(\"L\")\n",
        "        img2 = img2.convert(\"L\")\n",
        "            \n",
        "        if self.should_invert:\n",
        "            img0 = PIL.ImageOps.invert(img0)\n",
        "            img1 = PIL.ImageOps.invert(img1)\n",
        "            img2 = PIL.ImageOps.invert(img2)\n",
        "\n",
        "        print(img0)\n",
        "        if self.transform is not None:\n",
        "            img0 = self.transform(img0)\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "        print(img0)\n",
        "        print(img0.shape)\n",
        "        '''\n",
        "        print(\"anchor\")\n",
        "        print(img0_tuple[0])\n",
        "        print(\"pos\")\n",
        "        print(img1_tuple[0])\n",
        "        print(\"neg\")\n",
        "        print(img2_tuple[0])\n",
        "        '''\n",
        "\n",
        "        return (img0,\n",
        "                img1,\n",
        "                img2,\n",
        "                img0_tuple[0],\n",
        "                img1_tuple[0],\n",
        "                img2_tuple[0])\n",
        "\n",
        "\n",
        "class BatchTripletSiameseNetworkDataset(SiameseNetworkDataset):\n",
        "\n",
        "    # Returns an anchor image, a positive image (in the same class), and a \n",
        "    # negative image (in a different class)\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # First, we select a random image to be our anchor\n",
        "        randIndex = random.randrange(0, len(self.imageFolderDataset.imgs))\n",
        "        img0_tuple = self.imageFolderDataset.imgs[randIndex]\n",
        "        #print(img0_tuple)\n",
        "\n",
        "        img0 = Image.open(img0_tuple[0])\n",
        "        img0 = img0.convert(\"L\")\n",
        "            \n",
        "        if self.should_invert:\n",
        "            img0 = PIL.ImageOps.invert(img0)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img0 = self.transform(img0)\n",
        "        '''\n",
        "        print(\"anchor\")\n",
        "        print(img0_tuple[0])\n",
        "        print(\"pos\")\n",
        "        print(img1_tuple[0])\n",
        "        print(\"neg\")\n",
        "        print(img2_tuple[0])\n",
        "        '''\n",
        "\n",
        "        return (img0,\n",
        "                img0_tuple)\n",
        "\n",
        "        \n",
        "\n",
        "class TripletTestingSiameseNetworkDataset(SiameseNetworkDataset):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            imageFolderDataset,transform,should_invert,\n",
        "            anchor_index,unknown_iter_index=0,\n",
        "            anchor_file_identifier='',\n",
        "            worker_unique_id=0,\n",
        "            number_of_workers=1):\n",
        "        super().__init__(imageFolderDataset,transform,should_invert)\n",
        "        self.anchor_iter_index = anchor_index\n",
        "        self.unknown_iter_index = unknown_iter_index\n",
        "        self.anchor_file_identifier = anchor_file_identifier\n",
        "        self.worker_unique_id = worker_unique_id\n",
        "        self.number_of_workers = number_of_workers\n",
        "\n",
        "    # Returns an anchor image, a positive image (in the same class), and a \n",
        "    # negative image (in a different class)\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        #print(self.anchor_iter_index)\n",
        "        #print(\"before img0\")\n",
        "        img0_tuple = self.imageFolderDataset.imgs[self.anchor_iter_index.item()]\n",
        "        img1_tuple = self.imageFolderDataset.imgs[self.anchor_iter_index.item()]\n",
        "        #print(self.anchor_iter_index)\n",
        "        #print(type(img0_tuple))\n",
        "        #print(img0_tuple)\n",
        "        #print(\"after img0\")\n",
        "\n",
        "        # Secondly, we select a random image in the same class as our positive\n",
        "        #print(\"before img1\")\n",
        "        #img1 = generate_positive(img0_tuple)\n",
        "        #print(type(img1))\n",
        "        #print(img1)\n",
        "        #print(\"after img1\")\n",
        "\n",
        "        #print(self.imageFolderDataset.class_to_idx)\n",
        "              \n",
        "        while self.unknown_iter_index < len(self):\n",
        "            if (\n",
        "                    not self.anchor_file_identifier \n",
        "                    in self.imageFolderDataset.imgs[self.unknown_iter_index][0]\n",
        "                ):\n",
        "                img2_tuple = (\n",
        "                    self.imageFolderDataset.imgs[self.unknown_iter_index])\n",
        "                break\n",
        "            else:\n",
        "                self.unknown_iter_index += self.number_of_workers\n",
        "\n",
        "        if self.unknown_iter_index >= len(self):\n",
        "            self.unknown_iter_index = 0\n",
        "            return -1, self.worker_unique_id, -1, -1, -1, -1, -1\n",
        "        else:\n",
        "            self.unknown_iter_index += self.number_of_workers\n",
        "\n",
        "            #print(img0_tuple[0])\n",
        "            #print(img1_tuple[0])\n",
        "            img1 = generate_positive(img1_tuple[0])\n",
        "\n",
        "            img0 = Image.open(img0_tuple[0])\n",
        "            #img1 = Image.open(img1)\n",
        "            img2 = Image.open(img2_tuple[0])\n",
        "            img0 = img0.convert(\"L\")\n",
        "            img1 = img1.convert(\"L\")\n",
        "            img2 = img2.convert(\"L\")\n",
        "            \n",
        "            if self.should_invert:\n",
        "                img0 = PIL.ImageOps.invert(img0)\n",
        "                img1 = PIL.ImageOps.invert(img1)\n",
        "                img2 = PIL.ImageOps.invert(img2)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                img0 = self.transform(img0)\n",
        "                img1 = self.transform(img1)\n",
        "                img2 = self.transform(img2)\n",
        "            '''\n",
        "            print(\"anchor\")\n",
        "            print(img0_tuple[0])\n",
        "            print(\"pos\")\n",
        "            print(img1_tuple[0])\n",
        "            print(\"neg\")\n",
        "            print(img2_tuple[0])\n",
        "            '''\n",
        "\n",
        "            return (self.anchor_iter_index,\n",
        "                    self.worker_unique_id,\n",
        "                    img0, \n",
        "                    img1,\n",
        "                    img2,  \n",
        "                    img0_tuple[0],\n",
        "                    img2_tuple[0])\n",
        "\n",
        "        \n",
        "\n",
        "class PairwiseTestingSiameseNetworkDataset(SiameseNetworkDataset):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            imageFolderDataset,\n",
        "            transform,should_invert,\n",
        "            anchor_iter_index,\n",
        "            unknown_iter_index=0,\n",
        "            anchor_file_identifier='',\n",
        "            worker_unique_id=0,\n",
        "            number_of_workers=1):\n",
        "        super().__init__(imageFolderDataset,transform,should_invert)\n",
        "        self.anchor_iter_index = anchor_iter_index\n",
        "        self.unknown_iter_index = unknown_iter_index\n",
        "        self.anchor_file_identifier = anchor_file_identifier\n",
        "        self.worker_unique_id = worker_unique_id\n",
        "        self.number_of_workers = number_of_workers\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        #print(self.imageFolderDataset.classes)\n",
        "        while self.anchor_iter_index < len(self):\n",
        "            if (\n",
        "                    self.anchor_file_identifier \n",
        "                    in self.imageFolderDataset.imgs[self.anchor_iter_index][0]):\n",
        "                img0_tuple = (\n",
        "                    self.imageFolderDataset.imgs[self.anchor_iter_index])\n",
        "                break\n",
        "            else:\n",
        "                self.anchor_iter_index += self.number_of_workers\n",
        "\n",
        "        if self.anchor_iter_index >= len(self):\n",
        "            return -1, -1, -1, -1, -1, -1\n",
        "        else:\n",
        "            self.anchor_iter_index += self.number_of_workers\n",
        "            #print(self.anchor_iter_index)\n",
        "            img0 = Image.open(img0_tuple[0])\n",
        "            img0 = img0.convert(\"L\")\n",
        "            \n",
        "            if self.should_invert:\n",
        "                img0 = PIL.ImageOps.invert(img0)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                img0 = self.transform(img0)\n",
        "            #print(self.anchor_iter_index)\n",
        "            return (self.anchor_iter_index.item()-self.number_of_workers,\n",
        "                    self.worker_unique_id,\n",
        "                    img0,\n",
        "                    -1 ,\n",
        "                    img0_tuple[0],\n",
        "                    -1)\n",
        "\n",
        "\n",
        "class PairwiseTestingSiameseNetworkDataset2(SiameseNetworkDataset):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            imageFolderDataset,transform,should_invert,\n",
        "            anchor_iter_index,unknown_iter_index=0,\n",
        "            anchor_file_identifier='',\n",
        "            worker_unique_id=0,\n",
        "            number_of_workers=1):\n",
        "        super().__init__(imageFolderDataset,transform,should_invert)\n",
        "        self.anchor_iter_index = anchor_iter_index\n",
        "        self.unknown_iter_index = unknown_iter_index\n",
        "        self.anchor_file_identifier = anchor_file_identifier\n",
        "        self.worker_unique_id = worker_unique_id\n",
        "        self.number_of_workers = number_of_workers\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        img0_tuple = self.imageFolderDataset.imgs[self.anchor_iter_index]\n",
        "              \n",
        "        while self.unknown_iter_index < len(self):\n",
        "            if (\n",
        "                    not self.anchor_file_identifier \n",
        "                    in self.imageFolderDataset.imgs[self.unknown_iter_index][0]\n",
        "                ):\n",
        "                img1_tuple = (\n",
        "                    self.imageFolderDataset.imgs[self.unknown_iter_index])\n",
        "                break\n",
        "            else:\n",
        "                self.unknown_iter_index += self.number_of_workers\n",
        "\n",
        "        if self.unknown_iter_index >= len(self):\n",
        "            self.unknown_iter_index = 0\n",
        "            return -1, self.worker_unique_id, -1, -1, -1, -1\n",
        "        else:\n",
        "            self.unknown_iter_index += self.number_of_workers\n",
        "\n",
        "            img0 = Image.open(img0_tuple[0])\n",
        "            img1 = Image.open(img1_tuple[0])\n",
        "            img0 = img0.convert(\"L\")\n",
        "            img1 = img1.convert(\"L\")\n",
        "            \n",
        "            if self.should_invert:\n",
        "                img0 = PIL.ImageOps.invert(img0)\n",
        "                img1 = PIL.ImageOps.invert(img1)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                img0 = self.transform(img0)\n",
        "                img1 = self.transform(img1)\n",
        "\n",
        "            return (self.index,\n",
        "                    self.worker_unique_id,\n",
        "                    img0, \n",
        "                    img1 , \n",
        "                    img0_tuple[0], \n",
        "                    img1_tuple[0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k0ZQeUoA3-C",
        "colab_type": "text"
      },
      "source": [
        "## Setting the image folder to be used by the custom dataset\n",
        "\n",
        "In the first block, we specify the location of the training data. In the second block, we pass the images through the custom dataset class,  resizing them to 100 x 100 pixels, transforming them into tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gAhHc3_A3ED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_dataset = dset.ImageFolder(root=Config.training_dir)\n",
        "print(folder_dataset.imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqQBMCGIKdsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairwise_siamese_dataset = PairwiseTrainingSiameseNetworkDataset(\n",
        "    imageFolderDataset=folder_dataset,\n",
        "    transform=transforms.Compose([transforms.Resize((100,100)),\n",
        "                                  transforms.ToTensor()]),\n",
        "    should_invert=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qhy1C3CyuB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triplet_siamese_dataset = BatchTripletSiameseNetworkDataset(\n",
        "    imageFolderDataset=folder_dataset,\n",
        "    transform=transforms.Compose([transforms.Resize((100,100)),\n",
        "                                  transforms.ToTensor()]),\n",
        "    should_invert=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_KJxHsaBfU0",
        "colab_type": "text"
      },
      "source": [
        "## Visualising some of the training data\n",
        "\n",
        "The top row and the bottom row of any column is one pair. The 0s and 1s correspond to the column of the image. 1 indiciates dissimilar, and 0 indicates similar.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVBSCKdIKhFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vis_dataloader = DataLoader(pairwise_siamese_dataset,\n",
        "                        shuffle=True,\n",
        "                        num_workers=8,\n",
        "                        batch_size=8)\n",
        "dataiter = iter(vis_dataloader)\n",
        "\n",
        "\n",
        "example_batch = next(dataiter)\n",
        "concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
        "imshow(torchvision.utils.make_grid(concatenated))\n",
        "print(example_batch[2].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoUIsQJ5BpZp",
        "colab_type": "text"
      },
      "source": [
        "## Create the neural network\n",
        "\n",
        "We will use a standard convolutional neural network. Each convolutional layer has batch normalisation and then dropout. As Gupta says, 'There is nothing special about this network. It accepts an input of 100px by 100px and has 3 full connected layers after the convolution layers'. This might be where you want to experiment, eventually, with adding more layers and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qtvqaHjKjSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.cnn1 = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(1, 4, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            \n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(4, 8, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(8, 8, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(8*100*100, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(500, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(500, 5))\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        output = self.cnn1(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc1(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        return output1, output2\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjwdM0y93GE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.cnn1 = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(1, 4, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            \n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(4, 8, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(8, 8, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(8*100*100, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(500, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(500, 5))\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        output = self.cnn1(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc1(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, input1, input2, input3):\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        output3 = self.forward_once(input3)\n",
        "        return output1, output2, output3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJDzKsrpEI3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.cnn1 = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(1, 4, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            \n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(4, 8, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(8, 8, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(8*100*100, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(500, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(500, Config.train_batch_size))\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        output = self.cnn1(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc1(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.forward_once(input)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtETbBhICRle",
        "colab_type": "text"
      },
      "source": [
        "## Contrastive loss function\n",
        "\n",
        "Here we define the contrastive loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as_ZMSzVKnHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(\n",
        "            output1, output2, keepdim = True)\n",
        "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) + (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3wkl3zQ7pCs",
        "colab_type": "text"
      },
      "source": [
        "## Triplet loss function\n",
        "\n",
        "Here we define the triplet loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu8nQCEq7vBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TripletLoss(torch.nn.Module):\n",
        "\n",
        "    def forward(self, anchor_embed, pos_embed, neg_embed):\n",
        "        #print(anchor_embed.type())\n",
        "        #print(pos_embed.type())\n",
        "        #print(anchor_embed)\n",
        "        #print(anchor_embed**2)\n",
        "        d_pos = torch.sum((anchor_embed - pos_embed)**2, 1)\n",
        "        d_neg = torch.sum((anchor_embed - neg_embed)**2, 1)\n",
        "        zero = torch.zeros([1], dtype=torch.float32)\n",
        "        zero = zero.cuda()\n",
        "        #print(2.0 + (d_pos - d_neg))\n",
        "        loss_triplet = torch.max(zero, 1.0 + (d_pos - d_neg))\n",
        "        #print(loss_triplet)\n",
        "        loss_triplet = torch.mean(loss_triplet)\n",
        "        #print(loss_triplet)\n",
        "        return loss_triplet\n",
        "\n",
        "\n",
        "def _pairwise_distances(embeddings, squared=False):\n",
        "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "\n",
        "    Returns:\n",
        "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    # Get the dot product between all embeddings\n",
        "    # shape (batch_size, batch_size)\n",
        "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
        "\n",
        "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
        "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
        "    # shape (batch_size,)\n",
        "    square_norm = tf.diag_part(dot_product)\n",
        "\n",
        "    # Compute the pairwise distance matrix as we have:\n",
        "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
        "    # shape (batch_size, batch_size)\n",
        "    distances = tf.expand_dims(square_norm, 0) - 2.0 * dot_product + tf.expand_dims(square_norm, 1)\n",
        "\n",
        "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
        "    distances = tf.maximum(distances, 0.0)\n",
        "\n",
        "    if not squared:\n",
        "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
        "        # we need to add a small epsilon where distances == 0.0\n",
        "        mask = tf.to_float(tf.equal(distances, 0.0))\n",
        "        distances = distances + mask * 1e-16\n",
        "\n",
        "        distances = tf.sqrt(distances)\n",
        "\n",
        "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
        "        distances = distances * (1.0 - mask)\n",
        "\n",
        "    return distances\n",
        "\n",
        "\n",
        "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
        "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
        "\n",
        "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
        "\n",
        "    Args:\n",
        "        labels: labels of the batch, of size (batch_size,)\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        margin: margin for triplet loss\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "\n",
        "    Returns:\n",
        "        triplet_loss: scalar tensor containing the triplet loss\n",
        "    \"\"\"\n",
        "    # Get the pairwise distance matrix\n",
        "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
        "\n",
        "    # For each anchor, get the hardest positive\n",
        "    # First, we need to get a mask for every valid positive (they should have same label)\n",
        "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
        "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
        "\n",
        "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
        "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
        "\n",
        "    # shape (batch_size, 1)\n",
        "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
        "\n",
        "    # For each anchor, get the hardest negative\n",
        "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
        "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
        "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
        "\n",
        "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
        "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
        "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
        "\n",
        "    # shape (batch_size,)\n",
        "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
        "\n",
        "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
        "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
        "\n",
        "    # Get final mean triplet loss\n",
        "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
        "\n",
        "    return triplet_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIv7hO3UCdWf",
        "colab_type": "text"
      },
      "source": [
        "## Training the neural network\n",
        "\n",
        "The next three blocks configure all of the variables and settings for training the neural network. You might want to experiment with changing the values of the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or4xt6VDKp2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(\n",
        "    pairwise_siamese_dataset,\n",
        "    shuffle=False,\n",
        "    num_workers=Config.num_generator_workers,\n",
        "    batch_size=Config.train_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVLULZx0yhHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(\n",
        "    triplet_siamese_dataset,\n",
        "    shuffle=False,\n",
        "    num_workers=Config.num_generator_workers,\n",
        "    batch_size=1)\n",
        "\n",
        "net = SiameseNetwork().cuda()\n",
        "criterion = TripletLoss()\n",
        "optimizer = optim.Adam(net.parameters(),lr = 0.00006, betas=(0.9,0.999), eps=1e-8 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnRiUE3nKuqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = SiameseNetwork().cuda()\n",
        "criterion = ContrastiveLoss()\n",
        "optimizer = optim.Adam(net.parameters(),lr = 0.00006 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04O3bd3SCiXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counter = []\n",
        "loss_history = [] \n",
        "iteration_number= 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7YqWETMC1Zl",
        "colab_type": "text"
      },
      "source": [
        "### Start the training\n",
        "\n",
        "This next block will start the training for the number of epochs set at the start of the notebook in the configuration block. The code is slightly modified so that filenames get stored for the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA0y-hd7K5et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for epoch in range(0,Config.train_number_epochs):\n",
        "    for i, data in enumerate(train_dataloader,0):\n",
        "\t\t# Modifications by Tim Sherrat, to enable path printing\n",
        "        # Note the underscores! They're the path values, \n",
        "        # which we don't actually want here\n",
        "\t\t# Underscores are used as variable names for things we don't actually \n",
        "        # want\n",
        "        img0, img1 , label, _, _ = data\n",
        "        img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output1,output2 = net(img0,img1)\n",
        "        #print(output1)\n",
        "        #print(output2)\n",
        "        loss_contrastive = criterion(output1,output2,label)\n",
        "        loss_contrastive.backward()\n",
        "        optimizer.step()\n",
        "        if i %10 == 0 :\n",
        "            print(\"Epoch number {}\\n Current loss {}\\n\".format(\n",
        "                epoch,loss_contrastive.item()))\n",
        "            iteration_number +=10\n",
        "            counter.append(iteration_number)\n",
        "            loss_history.append(loss_contrastive.item())\n",
        "show_plot(counter,loss_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltCwmn8ayZGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(0,Config.train_number_epochs):\n",
        "    loss_triplet = 0\n",
        "    for i, data in enumerate(train_dataloader,0):\n",
        "\t\t# Modifications by Tim Sherrat, to enable path printing\n",
        "        # Note the underscores! They're the path values, \n",
        "        # which we don't actually want here\n",
        "\t\t# Underscores are used as variable names for things we don't actually \n",
        "        # want\n",
        "        img0, img1 , img2, _, _, _ = data\n",
        "        print(type(img0))\n",
        "        print(img0.shape)\n",
        "        img0, img1 , img2 = img0.cuda(), img1.cuda() , img2.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        anchor_output,pos_output,neg_output = net(img0,img1,img2)\n",
        "        #print(output1)\n",
        "        #print(output2)\n",
        "        loss_triplet = criterion(anchor_output,pos_output,neg_output)\n",
        "        #print(loss_triplet)\n",
        "        loss_triplet.backward()\n",
        "        optimizer.step()\n",
        "    print(\"Epoch number {}\\n Current loss {}\\n\".format(\n",
        "        epoch,loss_triplet.item()))\n",
        "    iteration_number +=train_dataloader.batch_size\n",
        "    counter.append(iteration_number)\n",
        "    loss_history.append(loss_triplet.item())\n",
        "show_plot(counter,loss_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uFcn4rz3u4pM",
        "colab": {}
      },
      "source": [
        "training_images_dataset = folder_dataset.imgs\n",
        "for epoch in range(0,Config.train_number_epochs):\n",
        "    batch_tensor = torch.Tensor(0,Config.train_batch_size).cuda()\n",
        "    batch_labels = torch.Tensor(0,).cuda()\n",
        "    selected_images = []\n",
        "    for i in range(0, Config.train_batch_size):\n",
        "        while(True):\n",
        "            #https://github.com/amdegroot/ssd.pytorch/issues/214\n",
        "            try:\n",
        "                potential_batch_sample, potential_name = next(batch_generator)\n",
        "            except:\n",
        "                batch_generator = iter(train_dataloader)\n",
        "                potential_batch_sample, potential_name = next(batch_generator)\n",
        "            if (potential_name[0] in selected_images):\n",
        "                continue\n",
        "            else:\n",
        "                selected_images.append(potential_name[0])\n",
        "                #print(potential_batch_sample[0])\n",
        "                #print(type(potential_batch_sample[0][0]))\n",
        "                #batch_sample = Image.open(potential_batch_sample[0][0])\n",
        "                #batch_sample = batch_sample.convert(\"L\")\n",
        "                #transformToTensor = transforms.Compose([transforms.Resize((100,100)), transforms.ToTensor()])\n",
        "                #batch_sample = transformToTensor(batch_sample)\n",
        "                #batch_sample\n",
        "                batch_sample = potential_batch_sample\n",
        "                #print(type(batch_sample))\n",
        "                #print(batch_sample.shape)\n",
        "                #print(batch_sample)\n",
        "                batch_sample = batch_sample.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                batch_sample_embed = net(batch_sample)\n",
        "                #print(type(batch_sample_embed))\n",
        "                #print(batch_sample_embed.shape)\n",
        "                #print(batch_sample_embed)\n",
        "                #if (i == 0):\n",
        "                #    batch_tensor = torch.cat((batch_tensor, batch_sample_embed), 0)\n",
        "                #else:\n",
        "                #    batch_tensor = torch.cat((batch_tensor, batch_sample_embed), 1)\n",
        "                #print(batch_sample_embed.shape)\n",
        "                #batch_sample_embed = batch_sample_embed.permute(1,0)\n",
        "                #print(batch_sample_embed.shape)\n",
        "                #print(batch_tensor.shape)\n",
        "                #print(batch_sample_embed)\n",
        "                batch_tensor = torch.cat((batch_tensor, batch_sample_embed), 0)\n",
        "                #print(potential_name[1][0])\n",
        "                batch_labels = torch.cat((batch_labels, ((potential_name[1]).float()).cuda()), 0)\n",
        "                #print(batch_labels)\n",
        "                #print(len(potential_label))\n",
        "                #print(potential_label[0])\n",
        "                #print(batch_tensor)\n",
        "                #print(batch_tensor.shape)\n",
        "                #print(batch_tensor)\n",
        "                break\n",
        "    print(batch_tensor.shape)\n",
        "    print(batch_tensor[0])\n",
        "    print(batch_labels.shape)\n",
        "    print(batch_labels)\n",
        "\n",
        "    hard_triplet_loss = batch_hard_triplet_loss(batch_labels, batch_tensor, margin=1.0)\n",
        "    hard_triplet_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    print(\"Next Epoch\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKVPWER5DJVS",
        "colab_type": "text"
      },
      "source": [
        "## Save the model\n",
        "\n",
        "The block below saves the state dictionary, and the model. This is handy so that once you *have* a trained model, you can return to it if your notebook connection to Colab is broken, or if you have to set the project aside for a while. The second block copies (`cp`) the file to a location on Google drive. You can also download the file to your machine directly by right-clicking the filename in the tray at left (you might need to hit 'refresh' to see it, first)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXTopDCEhyy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model!\n",
        "torch.save(net.state_dict(),'net_params.pkl')\n",
        "torch.save(net, 'net.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2QLT90P6eNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp net_params.pkl \"drive/My Drive/one-shot-test\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sizl8QETDvAg",
        "colab_type": "text"
      },
      "source": [
        "## Reloading the model at a later time\n",
        "\n",
        "If this is your first time through the notebook, you don't need to worry about this; skip down to **Testing**. If you're returning to the project, make sure\n",
        "+ that you've got hardware acceleration turned on\n",
        "+ that you've run all of the code again to import the necessary libraries, and set the various configurations \n",
        "  + _including_ the SiameseNetwork class\n",
        "\n",
        "The block below assumes you've connected Google drive. Alternatively you can upload directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCbuAtMOigQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the model back from your drive\n",
        "!cp \"drive/My Drive/one-shot-test/net_params.pkl\" data/net_params.pkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLQ_aEcCERPI",
        "colab_type": "text"
      },
      "source": [
        "...then tell the machine to load the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvKhEPhr7oqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the model\n",
        "net=SiameseNetwork()\n",
        "net.load_state_dict(torch.load('data/net_params.pkl'))\n",
        "dp = nn.DataParallel(net) #https://github.com/pytorch/pytorch/issues/3805\n",
        "\n",
        "# The incompatiblekeys message might not be an issue - see\n",
        "# https://gpytorch.readthedocs.io/en/latest/examples/00_Basic_Usage/Saving_and_Loading_Models.html \n",
        "# which replicates that incompatiblekeys message without any kind of comment, \n",
        "# seems to be hunkydory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LLQYb1LErDI",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "\n",
        "This block iteratively loads pairs of images with known provenance versus unknown provenance from different subfolders in your testing folder. It then compares the results of these anchor-unknown pairs using euclidean distance. It will print out the images with the dissimilarity distance, as well as printing out the filenames for each pair. You can experiment with printing these results to a file.\n",
        "\n",
        "Since this code currently outputs all anchor-unknown pairs, the runtime is __O(n^2)__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA4QpoRCLgSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Init variable representing testing data's directory, \n",
        "# see Config section to specify path\n",
        "folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\n",
        "\n",
        "# Create the dataset, pairwise_siamese_dataset_anchors, with data from the \n",
        "# testing data\n",
        "# Then, create a generator with pairwise_siamese_dataset-unknowns\n",
        "pairwise_siamese_dataset_anchors = PairwiseTestingSiameseNetworkDataset(\n",
        "    imageFolderDataset=folder_dataset_test,\n",
        "    transform=transforms.Compose([transforms.Resize((100,100)),\n",
        "                                transforms.ToTensor()]),\n",
        "    should_invert=False,\n",
        "    anchor_iter_index=0, \n",
        "    anchor_file_identifier='ref-')\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    pairwise_siamese_dataset_anchors,\n",
        "    num_workers=Config.num_generator_workers,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    worker_init_fn=worker_init_fn)\n",
        "\n",
        "generator_anchor_images = iter(test_dataloader)\n",
        "\n",
        "workers_terminated_outer = np.zeros(Config.num_generator_workers)\n",
        "# Outer loop searches for anchors\n",
        "for i in range(len(generator_anchor_images)):\n",
        "\n",
        "    # Returns a found anchor's index, image, and filepath\n",
        "    anchor_index,worker_id_outer,anchor_image,_,anchor_filepath,_ = next(\n",
        "        generator_anchor_images)\n",
        "    # Stop outer loop if index is out of bounds, no more potential anchors\n",
        "    if not(0 in workers_terminated_outer):\n",
        "        break\n",
        "\n",
        "    if anchor_index.item() < 0:\n",
        "        np.put(workers_terminated_outer, worker_id_outer, 1)\n",
        "    else:\n",
        "        # Create new dataset with a given known anchor's index\n",
        "        # Then, create a generator with pairwise_siamese_dataset-unknowns\n",
        "        pairwise_siamese_dataset_unknowns = PairwiseTestingSiameseNetworkDataset2(\n",
        "            imageFolderDataset=folder_dataset_test,\n",
        "            transform=transforms.Compose([transforms.Resize((100,100)),\n",
        "                                          transforms.ToTensor()]),\n",
        "            should_invert=False,\n",
        "            anchor_iter_index=anchor_index, \n",
        "            anchor_file_identifier='ref-')\n",
        "      \n",
        "        compare_dataloader = test_dataloader = DataLoader(\n",
        "            pairwise_siamese_dataset_unknowns,\n",
        "            num_workers=Config.num_generator_workers,\n",
        "            batch_size=1,shuffle=False,\n",
        "            worker_init_fn=worker_init_fn)\n",
        "      \n",
        "        generator_unknown_prov_images = iter(compare_dataloader)\n",
        "\n",
        "        workers_terminated_inner = np.zeros(Config.num_generator_workers)\n",
        "        # Inner loop pairs a known anchor with every image of unknown provenance\n",
        "        for k in range(len(generator_unknown_prov_images)):\n",
        "            # no_more_unknowns will return -1 if all pairs have been found\n",
        "            no_more_unknowns,worker_id_inner,_,unknown_prov_image,_,unknown_prov_filepath = (\n",
        "                next(generator_unknown_prov_images))\n",
        "\n",
        "            # Stop this inner loop if all anchor-unknown pairs for the current \n",
        "            # anchor image are found\n",
        "            if not(0 in workers_terminated_inner):\n",
        "                break\n",
        "\n",
        "            if no_more_unknowns.item() < 0:\n",
        "                np.put(workers_terminated_inner, worker_id_inner, 1)\n",
        "            else:\n",
        "                concatenated = torch.cat((anchor_image,unknown_prov_image),0)\n",
        "\n",
        "                # Again getting rid of cuda for my Mac\n",
        "                output1,output2 = net(\n",
        "                    Variable(anchor_image).cuda(),\n",
        "                    Variable(unknown_prov_image).cuda())\n",
        "                # output1,output2 = net(Variable(x0),Variable(x1))\n",
        "                euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "\n",
        "                # Show the images:\n",
        "                imshow(\n",
        "                    torchvision.utils.make_grid(concatenated),\n",
        "                    'Dissimilarity: {:.2f}'.format(euclidean_distance.item()))\n",
        "\n",
        "                # Show the paths for the two images\n",
        "                print('Image 1: {}'.format(anchor_filepath[0])) \n",
        "                print('Image 2: {}'.format(unknown_prov_filepath[0]))\n",
        "                print('Dissimilarity: {:.2f}'.format(euclidean_distance.item()))\n",
        "                # Print out the results as a kind of table.\n",
        "                # from tabulate import tabulate\n",
        "                # print(tabulate([[p0, p1,euclidean_distance.item()]], \n",
        "                    # headers=['image1', 'image2','euclidean_distance']))\n",
        "        \n",
        "        # Ensures each worker generates unique pair\n",
        "        # nth loop iteration \n",
        "        # * number of workers \n",
        "        # + this worker's ID (added in init)\n",
        "        #next_index_to_check = Config.num_generator_workers+anchor_index.item()\n",
        "        #anchor_index = torch.tensor(next_index_to_check)\n",
        "\n",
        "print('Finshed all anchor-unknown pair comparisons')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA-mKws0qzwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Init variable representing testing data's directory, \n",
        "# see Config section to specify path\n",
        "folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\n",
        "\n",
        "# Create the dataset, pairwise_siamese_dataset_anchors, with data from the \n",
        "# testing data\n",
        "# Then, create a generator with pairwise_siamese_dataset-unknowns\n",
        "triplet_siamese_dataset_anchors = PairwiseTestingSiameseNetworkDataset(\n",
        "    imageFolderDataset=folder_dataset_test,\n",
        "    transform=transforms.Compose([transforms.Resize((100,100)),\n",
        "                                transforms.ToTensor()]),\n",
        "    should_invert=False,\n",
        "    anchor_iter_index=0, \n",
        "    anchor_file_identifier='ref-')\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    triplet_siamese_dataset_anchors,\n",
        "    num_workers=Config.num_generator_workers,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    worker_init_fn=worker_init_fn)\n",
        "\n",
        "generator_anchor_images = iter(test_dataloader)\n",
        "\n",
        "workers_terminated_outer = np.zeros(Config.num_generator_workers)\n",
        "# Outer loop searches for anchors\n",
        "for i in range(len(generator_anchor_images)):\n",
        "\n",
        "    # Returns a found anchor's index, image, and filepath\n",
        "    ind_anchor,worker_id_outer,anchor_image,_,anchor_filepath,_ = next(\n",
        "        generator_anchor_images)\n",
        "    #print(anchor_index.item())\n",
        "    # Stop outer loop if index is out of bounds, no more potential anchors\n",
        "    if not(0 in workers_terminated_outer):\n",
        "        break\n",
        "\n",
        "    if ind_anchor < 0:\n",
        "        np.put(workers_terminated_outer, worker_id_outer, 1)\n",
        "    else:\n",
        "        #print(ind_anchor)\n",
        "        # Create new dataset with a given known anchor's index\n",
        "        # Then, create a generator with pairwise_siamese_dataset-unknowns\n",
        "        triplet_siamese_dataset_unknowns = TripletTestingSiameseNetworkDataset(\n",
        "            imageFolderDataset=folder_dataset_test,\n",
        "            transform=transforms.Compose([transforms.Resize((100,100)),\n",
        "                                          transforms.ToTensor()]),\n",
        "            should_invert=False,\n",
        "            anchor_index=ind_anchor, \n",
        "            anchor_file_identifier='ref-')\n",
        "        #print(ind_anchor.item())\n",
        "        #print(triplet_siamese_dataset_unknowns.anchor_iter_index)\n",
        "        triplet_siamese_dataset_unknowns.anchor_iter_index = ind_anchor.item()\n",
        "        #print(ind_anchor.item())\n",
        "        #print(triplet_siamese_dataset_unknowns.anchor_iter_index)\n",
        "        compare_dataloader = DataLoader(\n",
        "            triplet_siamese_dataset_unknowns,\n",
        "            num_workers=Config.num_generator_workers,\n",
        "            batch_size=1,shuffle=False,\n",
        "            worker_init_fn=worker_init_fn)\n",
        "      \n",
        "        generator_unknown_prov_images = iter(compare_dataloader)\n",
        "\n",
        "        workers_terminated_inner = np.zeros(Config.num_generator_workers)\n",
        "        # Inner loop pairs a known anchor with every image of unknown provenance\n",
        "        for k in range(len(generator_unknown_prov_images)):\n",
        "            # no_more_unknowns will return -1 if all pairs have been found\n",
        "            no_more_unknowns,worker_id_inner,_,_,unknown_prov_image,_,unknown_prov_filepath = (\n",
        "                next(generator_unknown_prov_images))\n",
        "            \n",
        "            #print(\"checking pos\")\n",
        "            #print(pos_generated_image)\n",
        "            #print(\"checked pos\")\n",
        "\n",
        "            # Stop this inner loop if all anchor-unknown pairs for the current \n",
        "            # anchor image are found\n",
        "            if not(0 in workers_terminated_inner):\n",
        "                break\n",
        "\n",
        "            if no_more_unknowns.item() < 0:\n",
        "                np.put(workers_terminated_inner, worker_id_inner, 1)\n",
        "            else:\n",
        "                #print(anchor_filepath[0])\n",
        "                \n",
        "                pos_generated_image = generate_positive(anchor_filepath[0])\n",
        "                pos_generated_image = pos_generated_image.convert(\"L\")\n",
        "                pos_generated_image = transforms.Compose([transforms.Resize((100,100)),\n",
        "                                          transforms.ToTensor()])(pos_generated_image)\n",
        "                pos_generated_image.unsqueeze_(0)\n",
        "                #print(pos_generated_image)\n",
        "                #print(anchor_image)\n",
        "                \n",
        "\n",
        "                #concatenated = torch.cat((anchor_image,pos_generated_image,unknown_prov_image),0)\n",
        "                concatenated = torch.cat((anchor_image,unknown_prov_image),0)\n",
        "\n",
        "                # Again getting rid of cuda for my Mac\n",
        "                anchor_embed,pos_embed,neg_embed = net(\n",
        "                    Variable(anchor_image).cuda(),\n",
        "                    Variable(pos_generated_image).cuda(),\n",
        "                    Variable(unknown_prov_image).cuda())\n",
        "                # output1,output2 = net(Variable(x0),Variable(x1))\n",
        "                #euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "                \n",
        "                '''\n",
        "                d_pos = torch.sum((anchor_embed - pos_embed)**2, 1)\n",
        "                d_neg = torch.sum((anchor_embed - neg_embed)**2, 1)\n",
        "                zero = torch.zeros([1], dtype=torch.float32)\n",
        "                zero = zero.cuda()\n",
        "                #print(2.0 + (d_pos - d_neg))\n",
        "                loss_triplet = torch.max(zero, 1.0 + (d_pos - d_neg))\n",
        "                #print(loss_triplet)\n",
        "                loss_triplet = torch.mean(loss_triplet)\n",
        "                '''\n",
        "                '''\n",
        "                euclidean_distance = anchor_embed - neg_embed\n",
        "                euclidean_distance = (euclidean_distance.cpu()).detach()\n",
        "                euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n",
        "                euclidean_distance = np.sqrt(euclidean_distance)\n",
        "                '''\n",
        "\n",
        "                euclidean_distance = F.pairwise_distance(anchor_embed, neg_embed)\n",
        "                cosine_similarity = F.cosine_similarity(anchor_embed, neg_embed)\n",
        "\n",
        "                #x = tf.constant(np.random.uniform(-1, 1, 10)) \n",
        "                #y = tf.constant(np.random.uniform(-1, 1, 10))\n",
        "                #s = tf.compat.v1.losses.cosine_distance(anchor_embed.cpu(), neg_embed.cpu(), dim=0)\n",
        "                #print(tf.Session().run(s))\n",
        "\n",
        "                #print(type(anchor_embed))\n",
        "\n",
        "                # Show the images:\n",
        "                imshow(\n",
        "                    torchvision.utils.make_grid(concatenated),\n",
        "                    'Euclidean Distance: {:.2f}'.format(euclidean_distance.item()),\n",
        "                    'Cosine Similarity: {:.4f}'.format(cosine_similarity.item()))\n",
        "\n",
        "                #print(cosine_similarity)\n",
        "                # Show the paths for the two images\n",
        "                print('Image 1: {}'.format(anchor_filepath[0])) \n",
        "                #print('Image 2: Generated Positive') \n",
        "                print('Image 3: {}'.format(unknown_prov_filepath[0]))\n",
        "                # Print out the results as a kind of table.\n",
        "                # from tabulate import tabulate\n",
        "                # print(tabulate([[p0, p1,euclidean_distance.item()]], \n",
        "                    # headers=['image1', 'image2','euclidean_distance']))\n",
        "        \n",
        "        # Ensures each worker generates unique pair\n",
        "        # nth loop iteration \n",
        "        # * number of workers \n",
        "        # + this worker's ID (added in init)\n",
        "        #next_index_to_check = Config.num_generator_workers+anchor_index.item()\n",
        "        #anchor_index = torch.tensor(next_index_to_check)\n",
        "\n",
        "print('Finshed all anchor-unknown pair comparisons')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVt-evEFFP62",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# The End\n",
        "\n",
        "The two code blocks below print out the structure of the neural network, and the version info of all of the loaded packages in this environment. This information is useful for replicating this notebook in the future.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDnSSRJpUuzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "model = net\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2x_rKjj7UxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# watermark is not installed by default.\n",
        "# the first time through, uncomment the two lines below\n",
        "# then run the block.\n",
        "#\n",
        "#\n",
        "# !pip install watermark\n",
        "# %load_ext watermark\n",
        "%watermark -v -m -p numpy,scipy,torchvision,PIL,tensorflow,torch -g"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}