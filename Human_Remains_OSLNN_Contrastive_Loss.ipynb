{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human Remains OSLNN - Contrastive Loss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AehLane/BoneTrade-Project-OSLNN/blob/master/Human_Remains_OSLNN_Contrastive_Loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWeTgO_e6Y9e",
        "colab_type": "text"
      },
      "source": [
        "# One Shot Learning with Siamese Networks\n",
        "\n",
        "Original code framework adapted from Harshvardhan Gupta, 2017 [One Shot Learning With Siamese Networks](https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/blob/master/Siamese-networks-medium.ipynb) with code snippets from Tim Sherrat and discussed in \n",
        "Graham, Lane, and Huffer, Can One-Shot Learning Identify Origins of Human Remains Shown on Social Media? \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3spdJLNajqKm",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "##Foundational Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQUMas9nMNv7",
        "colab_type": "text"
      },
      "source": [
        "For **novice users** new to data science or Google Colab, below are several links to resources hosted by 3rd parties that may help provide additional information or how-to instructions relevant to this Colab Notebook.*\n",
        "\n",
        "Google Colaboratory:\n",
        "*   Google's [\"Welcome to Colaboratory\"](https://colab.research.google.com/notebooks/welcome.ipynb)\n",
        "*   [Google Colaboratory FAQ](https://research.google.com/colaboratory/faq.html)\n",
        "*   Anne Bonner, 2019 [Getting Started With Google Colab, A Simple Tutotial for the Frustrated and Confused](https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c)\n",
        "*   Jason Richards, 2019 [Getting Local with Google Colab](https://medium.com/@jasonrichards911/getting-local-with-google-colab-a4d69f373364)\n",
        "\n",
        "Tensorflow & PyTorch:\n",
        "*   Jake VanderPlas, 2019 [Get started with Google Colaboratory (Coding TensorFlow)](https://www.youtube.com/watch?v=inN8seMm7UI&ab_channel=TensorFlow)\n",
        "*   Dr. Joanne Kitson, 2019 [Installing Tensorflow with CUDA, cuDNN and GPU support on Windows 10](https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781)\n",
        "\n",
        "Neural Networks & Data Science:\n",
        "*   Harshvardhan Gupta, 2017 [One Shot Learning With Siamese Networks](https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/blob/master/Siamese-networks-medium.ipynb)\n",
        "*   Will Koehrsen, 2018 [Neural Networks Embeddings Explained](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526)\n",
        "*   Sagar Sharma, 2017 [Epoch vs Batch Size vs Iterations](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)\n",
        "*   Raúl Gómez, 2019 [Understanding Ranking Loss, Contrastive Loss, Margin Loss, Triplet Loss, Hinge Loss and all those confusing names](https://gombru.github.io/2019/04/03/ranking_loss/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*We do not guarantee the quality of the content hosted by these 3rd parties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30S_KDoFnjZA",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "##Running this Notebook in Hosted vs. Local Runtime Environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4a0YuEXMdZo",
        "colab_type": "text"
      },
      "source": [
        "We are assuming you have loaded this notebook with Google Colab and that you are signed into a Google account. \n",
        "\n",
        "For the first time through, you will run each block of code in sequential order. If after having trained a network you wish to resume testing at a later date, we will show you how to save your model and reload it, at the appropriate blocks.\n",
        "\n",
        "By **default**, we are assuming **you will be using Colab's hosted runtime environment**, but if you have access to a server/cluster that you would prefer to use instead, or would prefer to run it locally on the machine that has loaded up this Colab notebook, read on:\n",
        "\n",
        "\n",
        "*   If you want to run this notebook in a **local environment on the machine that has loaded up this Colab notebook** or if you are using **Google's Compute Engine**, you can find instructions [here](https://research.google.com/colaboratory/local-runtimes.html)\n",
        "\n",
        "*   If you are looking to **run this notebook on another machine** and are **not** using **Google Compute Engine**, the following instructional guidelines may be of use to you.*\n",
        "\n",
        "*This is just how we have been running our local server instance. No guarantee it will work exactly as described for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF6fSt91yfkV",
        "colab_type": "text"
      },
      "source": [
        "###Our Server Settings and Specifications\n",
        "\n",
        "Server OS: Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-74-generic x86_64)\n",
        "\n",
        "GPU: Tesla V100-PCIE-16GB\n",
        "\n",
        "Compute capability: 7.0\n",
        "\n",
        "CUDA Toolkit: 10.1\n",
        "\n",
        "CuDnn: v7.6.5 for CUDA 10.1\n",
        "\n",
        "Python: 3.6.9\n",
        "\n",
        "Tensorflow: 2.1.0\n",
        "\n",
        "PyTorch: 1.4.0\n",
        "\n",
        "Jupyter Notebook: 6.0.3\n",
        "\n",
        "Additional libraries needed: imgaug, matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wswve6o7yrMX",
        "colab_type": "text"
      },
      "source": [
        "###Steps We Use to Connect to Our Server for Local Runtime\n",
        "\n",
        "1.   Open cmd prompt on the machine with the open Colab notebook.\n",
        "2. Create a port tunnel using ssh, so choose an open port on both the client and server side; `local_port` & `server_port`.\n",
        "3.   Choose one:\n",
        "\n",
        "    3a. If the server asks for a password on ssh connection `ssh -L localhost:local_port:localhost:server_port username:password@server_IP`.\n",
        "\n",
        "    3b.If the server asks for a password after ssh connection `ssh -L localhost:local_port:localhost:server_port username@server_IP`.\n",
        "4. Start the Jupyter notebook on the server_port chosen `jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --NotebookApp.port_retries=0 --notebook-dir=\"\" --no-browser --allow-root --NotebookApp.token='' --NotebookApp.disable_check_xsrf=True --port=server_port`.\n",
        "5. On another tab of the client side browser, check `localhost:local_port` to make sure the notebook has been correctly port forwarded.\n",
        "6. Select the drop-down menu for Colab runtimes in the top right of the notebook UI, 'Connect'.\n",
        "7. Select 'Connect to local runtime'.\n",
        "8. Enter `http://localhost:local_port/` into the pop-up window and select the 'Connect' button."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5oU-S9i-mkz",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNwIdkARMso-",
        "colab_type": "text"
      },
      "source": [
        "These imported libraries are required to build the neural network and display the results in the 'Testing' section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l92t8_-FOeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.utils\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import PIL.ImageOps\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpQfE1qVFUVW",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Hardware Acceleration (GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guk1SapEMw8M",
        "colab_type": "text"
      },
      "source": [
        "Under 'Edit' -> 'Notebook Settings' make sure to select 'GPU' for Hardware Acceleration. This block of code confirms that a GPU has been selected for Tensorflow operations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjVyWDJAF4-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egql6oG4BGpk",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "##Loading Data for Training & Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSG-dedSM1SO",
        "colab_type": "text"
      },
      "source": [
        "**By default, we expect the data to be stored as a compressed .zip directory with the following structure:**\n",
        "\n",
        "    data.zip/\n",
        "    |----testing/\n",
        "    |    |----test_img_1\n",
        "    |    |    |----test_img_1.png\n",
        "    |    |----test_img_2\n",
        "    |    |    |----test_img_2.png\n",
        "    |    |----test_img_3\n",
        "    |    |    |----test_img_3.png\n",
        "    |    |----...\n",
        "    |----training/\n",
        "    |    |----train_img_1/\n",
        "    |    |    |----train_img_1_augmented_variation_1.png\n",
        "    |    |    |----train_img_1_augmented_variation_2.png\n",
        "    |    |    |----train_img_1_augmented_variation_3.png\n",
        "    |    |    |----...\n",
        "    |    |----train_img_2/\n",
        "    |    |    |----train_img_2_augmented_variation_1.png\n",
        "    |    |    |----train_img_2_augmented_variation_2.png\n",
        "    |    |    |----train_img_2_augmented_variation_3.png\n",
        "    |    |    |----...\n",
        "    |    |----train_img_3/\n",
        "    |    |    |----train_img_3_augmented_variation_1.png\n",
        "    |    |    |----train_img_3_augmented_variation_2.png\n",
        "    |    |    |----train_img_3_augmented_variation_3.png\n",
        "    |    |    |----...\n",
        "\n",
        "The methods to upload the data into this notebook depend upon the runtime environment type chosen; hosted runtime or local runtime. Both hosted and local runtimes have their own instructions and associated code outlined below in their respective following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrQFGF1T8wgI",
        "colab_type": "text"
      },
      "source": [
        "### Load Data (Hosted Runtime)\n",
        "\n",
        "Loading data for a hosted runtime may be done by uploading the zipped data folder directly or via a Google Drive account which contains the 'data.zip' by mounting the account.\n",
        "\n",
        "**To upload directly**, open the tray at left by clicking on the `>` button. Select 'Files' and then 'Upload'. Then, select a zip file in the file explorer from the local machine. If the tray does not refresh automatically, hit 'refresh' to update the tray to show changes.\n",
        "\n",
        "Unzip the data by running the next block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byy27-Z_EHgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If I'm uploading directly\n",
        "!unzip data.zip -d data/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp4KELeH9wcp",
        "colab_type": "text"
      },
      "source": [
        " **To mount Google Drive**, run the next block of code. The results block will display a URL. Click on this URL, and a new window will open asking for confirmation to connect Google Drive by allowing the listed permissions. Once confirmed, an authorization code will be displayed. Copy this code, and paste into the results block below. If all goes well, the results block will shortly display the text, 'Mounted at /content/drive'. Refresh the files pane in the tray at left.\n",
        "\n",
        "If the Colab Notebook successfully connected to the Google Drive account, it will appear as a folder within the tray. To change the location in the tray Google Drive is mounted, right-click on a folder within Google Drive, copy the path, and then paste that into the code below. **Note**: The leading `/content/` may need to be deleted from the path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne9mUU_TQFLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ft2c2VM-Zx7",
        "colab_type": "text"
      },
      "source": [
        "The code cell below shows how to copy a file from Google Drive to this space, and then unzip the folder. Optionally, uncommenting `!mkdir data && unzip data.zip -d data/` will create a new directory to contain the unzipped data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0rfcjBnQbVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you have data already on google drive\n",
        "!cp \"drive/My Drive/one-shot-test/data.zip\" data.zip\n",
        "\n",
        "# !mkdir data && unzip data.zip -d data/\n",
        "!unzip data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRgIwYGu9qV_",
        "colab_type": "text"
      },
      "source": [
        "### Load Data (Local Runtime)\n",
        "\n",
        "Loading data for a local runtime may be performed by the following code cell which will unzip the data folder. Optionally, it is possible to specify the path to the 'data.zip' below, ie. `path/to/your/data.zip`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvPOjTc6dgWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# https://thispointer.com/python-how-to-unzip-a-file-extract-single-multiple-or-all-files-from-a-zip-archive/\n",
        "# Create a ZipFile Object and load sample.zip in it\n",
        "with ZipFile('data.zip', 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall('data')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0yyYaOq-031",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFKH4yfnM5b0",
        "colab_type": "text"
      },
      "source": [
        "Three helper functions, `imshow` and `show_plot`, and `worker_init_fn`, are defined here to assist in parallelizing or displaying results in later sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE-TfRPnGGxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Is called when we want to show our compared images in the testing output\n",
        "def imshow(img, text=None, text2=None, should_save=False):\n",
        "    npimg = img.numpy()\n",
        "    plt.axis(\"off\")\n",
        "    if text:\n",
        "        plt.text(\n",
        "            10, 10, text, style='italic', fontweight='bold',\n",
        "            bbox={'facecolor': 'white', 'alpha': 0.8, 'pad': 10})\n",
        "    if text2:\n",
        "        plt.text(\n",
        "            120, 10, text2, style='italic', fontweight='bold',\n",
        "            bbox={'facecolor': 'white', 'alpha': 0.8, 'pad': 10})\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Simple display function used to show loss during training\n",
        "def show_plot(iteration, loss):\n",
        "    plt.plot(iteration, loss)\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OMg3RNu-QsV",
        "colab_type": "text"
      },
      "source": [
        "### Defining Dataloader Workers' init()\n",
        "\n",
        "This function, `worker_init_fn` allows splitting the task of generating anchor-unknown pairs among a group of workers. This function is provided as an argument for `DataLoader` class types, giving the dataloader an init method for its workers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dv5acBUGTpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def worker_init_fn(worker_id):\n",
        "    worker_info = torch.utils.data.get_worker_info()\n",
        "    dataset = worker_info.dataset\n",
        "\n",
        "    dataset.worker_unique_id = worker_info.id\n",
        "    dataset.number_of_workers = worker_info.num_workers\n",
        "    dataset.anchor_iter_index = torch.tensor(worker_info.id)\n",
        "    dataset.unknown_iter_index = torch.tensor(worker_info.id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54yR3gxVEp3c",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "##Custom Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zQ-2g6TM9Or",
        "colab_type": "text"
      },
      "source": [
        "Custom class definitions including the `Config` class, `SiameseNetworkDataset` classes, and our neural network's `SiameseNetwork` class are defined here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hh2Y_l8_bpY",
        "colab_type": "text"
      },
      "source": [
        "### Configuration Class\n",
        "\n",
        "The 'Config' class defines variables that will be reused throughout later sections.\n",
        "\n",
        "You may want to change some of these variables to work for your purposes.\n",
        "\n",
        "Refer to the **'Loading Data' section for guidelines** for directory structure for the loaded data.\n",
        "\n",
        "`training_dir = \"path/to/your/data/training`\n",
        "\n",
        "`testing_dir = \"path/to/your/data/testing`\n",
        "\n",
        "`num_generator_workers = desired_number_of_parallel_workers`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwjIJqCwdy_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config():\n",
        "    training_dir = \"data/data/training\"\n",
        "    testing_dir = \"data/data/testing\"\n",
        "    train_batch_size = 32\n",
        "    train_number_epochs = 255\n",
        "    num_generator_workers = 8\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC_2Oc5L_8V4",
        "colab_type": "text"
      },
      "source": [
        "### Custom Dataset Classes\n",
        "\n",
        "Under a superclass of `SiameseNetworkDataset`, three subclasses exist for different purposes; one for training and two for testing.\n",
        "\n",
        "**For training**, `TrainingSiameseNetworkDataset`'s `__getitem__` method generates a pair of images from the training dataset using `should_get_same_class` to determine the type of pair to generate; 0 for a same class pair and 1 for a different class pair when called. The generator takes images from the training directory specified in `Config` under the assumption that images within a subfolder are of the same class and images from different subfolders are of different classes. The addition to the original code, the class implementation now residing under `TrainingSiameseNetworkDataset`, is courtesy of Tim Sherratt with modifications by Alex Lane.\n",
        "\n",
        "**Note:** If the training dataset subdirectories do not have the same number of images in each subdirectory, `TrainingSiameseNetworkDataset` will introduce a bias; images withhin subfolders with less images have a higher probability of being selected. Over a large number of pairs, more pairs containing images from subdirectories with less images will be generated. It is advised that the subdirectories have a uniform number of images within to ensure random selection.\n",
        "\n",
        "**For testing**, `TestingSiameseNetworkDatasetAnchors`' `__getitem__` method returns a valid anchor while `TestingSiameseNetworkDatasetUnknowns`' `__getitem__` method returns a valid unknown together forming a valid anchor-unknown pair of images. Since the data is kept as a generator due to possibly working with large sets of data, the runtime should be __O(n^2)__ in the 'Testing' section due to iteratively comparing each element of the dataset to each potential pair member in the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C49BrRm0GdTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetworkDataset(Dataset):\n",
        "\n",
        "    def __init__(self, imageFolderDataset, transform=None, should_invert=True):\n",
        "        self.imageFolderDataset = imageFolderDataset\n",
        "        self.transform = transform\n",
        "        self.should_invert = should_invert\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imageFolderDataset.imgs)\n",
        "\n",
        "\n",
        "class TrainingSiameseNetworkDataset(SiameseNetworkDataset):\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_tuple = random.choice(self.imageFolderDataset.imgs)\n",
        "\n",
        "        # We need to make sure approx 50% of images are in the same class\n",
        "        should_get_same_class = bool(random.getrandbits(1))\n",
        "\n",
        "        num_pos_images = (\n",
        "            len(self.imageFolderDataset.imgs) /\n",
        "            len(self.imageFolderDataset.class_to_idx))\n",
        "\n",
        "        if should_get_same_class:\n",
        "            # Keep looping till the same class image is found\n",
        "            while True:\n",
        "                potential_positive_tuple = self.imageFolderDataset.imgs[\n",
        "                    random.randrange(\n",
        "                        (anchor_tuple[1] * num_pos_images),\n",
        "                        ((anchor_tuple[1] + 1) * num_pos_images)\n",
        "                        )]\n",
        "\n",
        "                if (anchor_tuple != potential_positive_tuple):\n",
        "                    positive_tuple = potential_positive_tuple\n",
        "                    break\n",
        "\n",
        "            anchor_image = Image.open(anchor_tuple[0])\n",
        "            positive_image = Image.open(positive_tuple[0])\n",
        "            anchor_image = anchor_image.convert(\"L\")\n",
        "            positive_image = positive_image.convert(\"L\")\n",
        "\n",
        "            if self.should_invert:\n",
        "                anchor_image = PIL.ImageOps.invert(anchor_image)\n",
        "                positive_image = PIL.ImageOps.invert(positive_image)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                anchor_image = self.transform(anchor_image)\n",
        "                positive_image = self.transform(positive_image)\n",
        "\n",
        "            return (anchor_image,\n",
        "                    positive_image,\n",
        "                    torch.from_numpy(\n",
        "                        np.array(\n",
        "                            [int(positive_tuple[1] != anchor_tuple[1])],\n",
        "                            dtype=np.float32)),\n",
        "                    anchor_tuple[0],\n",
        "                    positive_tuple[0])\n",
        "\n",
        "        else:\n",
        "            size_first_neg_range = anchor_tuple[1]*num_pos_images\n",
        "            size_second_neg_range = len(self.imageFolderDataset.imgs) - (\n",
        "                (anchor_tuple[1] + 1) * num_pos_images)\n",
        "\n",
        "            total_random_neg_indices = size_first_neg_range + (\n",
        "                size_second_neg_range)\n",
        "            neg_index_chosen = int(total_random_neg_indices * random.random())\n",
        "\n",
        "            if(neg_index_chosen < size_first_neg_range):\n",
        "                negative_tuple = self.imageFolderDataset.imgs[int(\n",
        "                    neg_index_chosen)]\n",
        "            else:\n",
        "                negative_tuple = self.imageFolderDataset.imgs[int(\n",
        "                    neg_index_chosen +\n",
        "                    num_pos_images)]\n",
        "\n",
        "            anchor_image = Image.open(anchor_tuple[0])\n",
        "            negative_image = Image.open(negative_tuple[0])\n",
        "            anchor_image = anchor_image.convert(\"L\")\n",
        "            negative_image = negative_image.convert(\"L\")\n",
        "\n",
        "            if self.should_invert:\n",
        "                anchor_image = PIL.ImageOps.invert(anchor_image)\n",
        "                negative_image = PIL.ImageOps.invert(negative_image)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                anchor_image = self.transform(anchor_image)\n",
        "                negative_image = self.transform(negative_image)\n",
        "        \n",
        "            return (anchor_image,\n",
        "                    negative_image,\n",
        "                    torch.from_numpy(\n",
        "                        np.array(\n",
        "                            [int(negative_tuple[1] != anchor_tuple[1])],\n",
        "                            dtype=np.float32)),\n",
        "                    anchor_tuple[0],\n",
        "                    negative_tuple[0])\n",
        "\n",
        "\n",
        "class TestingSiameseNetworkDatasetAnchors(SiameseNetworkDataset):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            imageFolderDataset,\n",
        "            transform, should_invert,\n",
        "            anchor_iter_index,\n",
        "            unknown_iter_index=0,\n",
        "            anchor_file_identifier='',\n",
        "            worker_unique_id=0,\n",
        "            number_of_workers=1):\n",
        "        super().__init__(imageFolderDataset, transform, should_invert)\n",
        "        self.anchor_iter_index = anchor_iter_index\n",
        "        self.unknown_iter_index = unknown_iter_index\n",
        "        self.anchor_file_identifier = anchor_file_identifier\n",
        "        self.worker_unique_id = worker_unique_id\n",
        "        self.number_of_workers = number_of_workers\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        while self.anchor_iter_index < len(self):\n",
        "            if (\n",
        "                    self.anchor_file_identifier\n",
        "                    in\n",
        "                    self.imageFolderDataset.imgs[self.anchor_iter_index][0]):\n",
        "                anchor_tuple = (\n",
        "                    self.imageFolderDataset.imgs[self.anchor_iter_index])\n",
        "                break\n",
        "            else:\n",
        "                self.anchor_iter_index += self.number_of_workers\n",
        "\n",
        "        if self.anchor_iter_index >= len(self):\n",
        "            return -1, -1, -1, -1, -1, -1\n",
        "        else:\n",
        "            self.anchor_iter_index += self.number_of_workers\n",
        "            anchor_image = Image.open(anchor_tuple[0])\n",
        "            anchor_image = anchor_image.convert(\"L\")\n",
        "\n",
        "            if self.should_invert:\n",
        "                anchor_image = PIL.ImageOps.invert(anchor_image)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                anchor_image = self.transform(anchor_image)\n",
        "\n",
        "            return (self.anchor_iter_index.item()-self.number_of_workers,\n",
        "                    self.worker_unique_id,\n",
        "                    anchor_image,\n",
        "                    -1,\n",
        "                    anchor_tuple[0],\n",
        "                    -1)\n",
        "\n",
        "\n",
        "class TestingSiameseNetworkDatasetUnknowns(SiameseNetworkDataset):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            imageFolderDataset, transform, should_invert,\n",
        "            anchor_iter_index, unknown_iter_index=0,\n",
        "            anchor_file_identifier='',\n",
        "            worker_unique_id=0,\n",
        "            number_of_workers=1):\n",
        "        super().__init__(imageFolderDataset, transform, should_invert)\n",
        "        self.anchor_iter_index = anchor_iter_index\n",
        "        self.unknown_iter_index = unknown_iter_index\n",
        "        self.anchor_file_identifier = anchor_file_identifier\n",
        "        self.worker_unique_id = worker_unique_id\n",
        "        self.number_of_workers = number_of_workers\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        anchor_tuple = self.imageFolderDataset.imgs[self.anchor_iter_index]\n",
        "\n",
        "        while self.unknown_iter_index < len(self):\n",
        "            if (\n",
        "                    self.anchor_file_identifier\n",
        "                    in self.imageFolderDataset.imgs[self.unknown_iter_index][0]\n",
        "                    ):\n",
        "                self.unknown_iter_index += self.number_of_workers\n",
        "            else:\n",
        "                unknown_tuple = (\n",
        "                    self.imageFolderDataset.imgs[self.unknown_iter_index])\n",
        "                break\n",
        "\n",
        "        if self.unknown_iter_index >= len(self):\n",
        "            return -1, self.worker_unique_id, -1, -1, -1, -1\n",
        "        else:\n",
        "            self.unknown_iter_index += self.number_of_workers\n",
        "\n",
        "            anchor_image = Image.open(anchor_tuple[0])\n",
        "            unknown_image = Image.open(unknown_tuple[0])\n",
        "            anchor_image = anchor_image.convert(\"L\")\n",
        "            unknown_image = unknown_image.convert(\"L\")\n",
        "\n",
        "            if self.should_invert:\n",
        "                anchor_image = PIL.ImageOps.invert(anchor_image)\n",
        "                unknown_image = PIL.ImageOps.invert(unknown_image)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                anchor_image = self.transform(anchor_image)\n",
        "                unknown_image = self.transform(unknown_image)\n",
        "\n",
        "            return (self.unknown_iter_index,\n",
        "                    self.worker_unique_id,\n",
        "                    anchor_image,\n",
        "                    unknown_image,\n",
        "                    anchor_tuple[0],\n",
        "                    unknown_tuple[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoUIsQJ5BpZp",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network Class\n",
        "\n",
        "Defined below is a standard convolutional neural network. Each convolutional layer has batch normalisation and then dropout. As Gupta says, 'There is nothing special about this network. It accepts an input of 100px by 100px and has 3 full connected layers after the convolution layers'. Optionally, to further experiment, adding or modifying of layers may be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qtvqaHjKjSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.cnn1 = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(1, 4, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(4, 8, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(8, 8, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(8*100*100, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(500, 500),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(500, 5))\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        output = self.cnn1(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc1(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "        return output1, output2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k0ZQeUoA3-C",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Setting Up the Training Dataset and Associated Image Folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlQHHxrRNF6h",
        "colab_type": "text"
      },
      "source": [
        "The location of the training data is set below to the variable defined by 'Config'. In the second block, the images are provided as a parameter to the custom dataset class, `TrainingSiameseNetworkDataset`, which resizes them to 100 x 100 pixels and transforming them into tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gAhHc3_A3ED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_dataset = dset.ImageFolder(root=Config.training_dir)\n",
        "# print(folder_dataset.imgs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqQBMCGIKdsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "siamese_dataset = TrainingSiameseNetworkDataset(\n",
        "    imageFolderDataset=folder_dataset,\n",
        "    transform=transforms.Compose([transforms.Resize((100, 100)),\n",
        "                                  transforms.ToTensor()]),\n",
        "    should_invert=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_KJxHsaBfU0",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Visualising the Training Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDcZVaFpNI3s",
        "colab_type": "text"
      },
      "source": [
        "The top row and the bottom row of any column is one pair. The 0s and 1s correspond to the column of the image. 1 indiciates dissimilar, and 0 indicates similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVBSCKdIKhFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vis_dataloader = DataLoader(\n",
        "    siamese_dataset, shuffle=True, num_workers=1, batch_size=8)\n",
        "dataiter = iter(vis_dataloader)\n",
        "\n",
        "example_batch = next(dataiter)\n",
        "concatenated = torch.cat((example_batch[0], example_batch[1]), 0)\n",
        "imshow(torchvision.utils.make_grid(concatenated))\n",
        "print(example_batch[2].numpy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYNqH02sHdlU",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "##Create or Load a Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gThcojuLNLA6",
        "colab_type": "text"
      },
      "source": [
        "Here, a new neural network model can be trained from scratch, or alternatively skip ahead to upload a pre-existing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iFjQUx_QeJRn"
      },
      "source": [
        "### Training a New Neural Network Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtETbBhICRle",
        "colab_type": "text"
      },
      "source": [
        "#### Contrastive Loss Function\n",
        "\n",
        "The defined contrastive loss function below will be used as the criterion by which the network weights are later updated during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as_ZMSzVKnHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(\n",
        "            output1, output2, keepdim=True)\n",
        "        loss_contrastive = torch.mean(\n",
        "            (1-label) * torch.pow(euclidean_distance, 2) +\n",
        "            (label) * torch.pow(\n",
        "                torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJyrP4VAPlqs",
        "colab_type": "text"
      },
      "source": [
        "####Setting Up the Network for Training\n",
        "\n",
        "The next three blocks configure all of the variables and settings for training the neural network. Optionally, the values of the Adam optimizer may be tweaked for further experimentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or4xt6VDKp2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(\n",
        "    siamese_dataset,\n",
        "    shuffle=False,\n",
        "    num_workers=Config.num_generator_workers,\n",
        "    batch_size=Config.train_batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnRiUE3nKuqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = SiameseNetwork().cuda()\n",
        "criterion = ContrastiveLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.00006)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04O3bd3SCiXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counter = []\n",
        "loss_history = []\n",
        "iteration_number = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7YqWETMC1Zl",
        "colab_type": "text"
      },
      "source": [
        "#### Training the Neural Network\n",
        "\n",
        "This next block will start the training for the number of epochs set at the start of the notebook in the configuration block. The code is slightly modified so that filenames get stored for the images. Approximately half of the anchor-unknown pairs will be anchor-positive and anchor-negative pairs respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA0y-hd7K5et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(0, Config.train_number_epochs):\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # Modifications by Tim Sherrat, to enable path printing\n",
        "        # Note the underscores! They're the path values,\n",
        "        # which we don't actually want here\n",
        "        # Underscores are used as variable names for things we don't actually\n",
        "        # want\n",
        "        anchor_image, unknown_image, label, _, _ = data\n",
        "        anchor_image, unknown_image, label = anchor_image.cuda(), (\n",
        "            unknown_image.cuda()), label.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embedding, unknown_embedding = net(anchor_image, unknown_image)\n",
        "        loss_contrastive = criterion(\n",
        "            anchor_embedding, unknown_embedding, label)\n",
        "        loss_contrastive.backward()\n",
        "        optimizer.step()\n",
        "        if i % 10 == 0:\n",
        "            print(\"Epoch number {}\\n Current loss {}\\n\".format(\n",
        "                epoch, loss_contrastive.item()))\n",
        "            iteration_number += 10\n",
        "            counter.append(iteration_number)\n",
        "            loss_history.append(loss_contrastive.item())\n",
        "show_plot(counter, loss_history)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKVPWER5DJVS",
        "colab_type": "text"
      },
      "source": [
        "### Saving the Neural Network Model & State Dictionary\n",
        "\n",
        "The block below saves the state dictionary, and the model. Then, it is possible to return to it if the notebook connection to Colab is broken, or if the project is set aside for a time. The second block copies `cp` the file to a location on Google drive. It is also possible to download the file to the local machine directly by right-clicking the filename in the tray at left (hit 'refresh' to update changes if the model does not at first appear)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXTopDCEhyy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model!\n",
        "torch.save(net.state_dict(), 'net_params_new.pkl')\n",
        "torch.save(net, 'net.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2QLT90P6eNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp net_params.pkl \"drive/My Drive/one-shot-test\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sizl8QETDvAg",
        "colab_type": "text"
      },
      "source": [
        "### Loading a Neural Network Model & State Dictionary\n",
        "\n",
        "The first time through this notebook, this section is not important; skip down to 'Testing'. Otherwise, upon returning to the project make sure that **sections 'Imports' through 'Setting the image folder...' are run**.\n",
        "\n",
        "This code cell below assumes Google Drive is mounted & connected. Alternatively, a model may be directly uploaded or load a model from the local machine using the second code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCbuAtMOigQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the model back from your drive\n",
        "!cp \"drive/My Drive/one-shot-test/net_params.pkl\" net_params.pkl\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLQ_aEcCERPI",
        "colab_type": "text"
      },
      "source": [
        "...then tell the machine to load the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvKhEPhr7oqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the model\n",
        "net = SiameseNetwork()\n",
        "net.load_state_dict(torch.load('net_params.pkl'))\n",
        "dp = nn.DataParallel(net)  # https://github.com/pytorch/pytorch/issues/3805\n",
        "\n",
        "# The incompatiblekeys message might not be an issue - see\n",
        "# https://gpytorch.readthedocs.io/en/latest/examples/00_Basic_Usage/Saving_and_Loading_Models.html\n",
        "# which replicates that incompatiblekeys message without any kind of comment,\n",
        "# seems to be hunkydory\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LLQYb1LErDI",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVB-9MnZNOi_",
        "colab_type": "text"
      },
      "source": [
        "This block iteratively loads pairs of images with known provenance versus unknown provenance from different subfolders in the testing folder. It then compares the results of these anchor-unknown pairs using euclidean distance. It will print out the images with the dissimilarity (euclidean distance), as well as printing out the filenames for each pair.\n",
        "\n",
        "Since this code currently outputs all possible anchor-unknown pairs in the testing folder specified in `Config`, this results in the runtime being **O(n^2)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA4QpoRCLgSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Time code snippet\n",
        "# https://stackoverflow.com/questions/1557571/how-do-i-get-time-of-a-python-programs-execution\n",
        "start_time = time.time()\n",
        "\n",
        "# Init variable representing testing data's directory,\n",
        "# see Config section to specify path\n",
        "folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\n",
        "\n",
        "# Create the dataset, siamese_dataset_anchors, with data from the\n",
        "# testing data\n",
        "# Then, create a generator with siamese_dataset-unknowns\n",
        "siamese_dataset_anchors = TestingSiameseNetworkDatasetAnchors(\n",
        "    imageFolderDataset=folder_dataset_test,\n",
        "    transform=transforms.Compose(\n",
        "        [transforms.Resize((100, 100)), transforms.ToTensor()]),\n",
        "    should_invert=False,\n",
        "    anchor_iter_index=0,\n",
        "    anchor_file_identifier='ref-')\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    siamese_dataset_anchors,\n",
        "    num_workers=Config.num_generator_workers,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    worker_init_fn=worker_init_fn)\n",
        "\n",
        "generator_anchor_images = iter(test_dataloader)\n",
        "workers_terminated_outer = np.zeros(Config.num_generator_workers)\n",
        "\n",
        "# Outer loop searches for anchors\n",
        "for i in range(len(generator_anchor_images)):\n",
        "\n",
        "    # Returns a found anchor's index, image, and filepath\n",
        "    anchor_index, worker_id_outer, anchor_image, _, anchor_filepath, _ = next(\n",
        "        generator_anchor_images)\n",
        "\n",
        "    # Stop outer loop if index is out of bounds, no more potential anchors\n",
        "    if not(0 in workers_terminated_outer):\n",
        "        break\n",
        "\n",
        "    if anchor_index.item() < 0:\n",
        "        np.put(workers_terminated_outer, worker_id_outer, 1)\n",
        "        if not(0 in workers_terminated_outer):\n",
        "            break\n",
        "    else:\n",
        "\n",
        "        # Create new dataset with a given known anchor's index\n",
        "        # Then, create a generator with siamese_dataset-unknowns\n",
        "        siamese_dataset_unknowns = (\n",
        "            TestingSiameseNetworkDatasetUnknowns(\n",
        "            imageFolderDataset=folder_dataset_test,\n",
        "            transform=transforms.Compose([transforms.Resize((100, 100)),\n",
        "                                          transforms.ToTensor()]),\n",
        "            should_invert=False,\n",
        "            anchor_iter_index=anchor_index,\n",
        "            anchor_file_identifier='ref-'))\n",
        "\n",
        "        compare_dataloader = test_dataloader = DataLoader(\n",
        "            siamese_dataset_unknowns,\n",
        "            num_workers=Config.num_generator_workers,\n",
        "            batch_size=1, shuffle=False,\n",
        "            worker_init_fn=worker_init_fn)\n",
        "\n",
        "        generator_unknown_prov_images = iter(compare_dataloader)\n",
        "        workers_terminated_inner = np.zeros(Config.num_generator_workers)\n",
        "\n",
        "        # Inner loop pairs known anchor with every image of unknown provenance\n",
        "        for k in range(len(generator_unknown_prov_images)):\n",
        "\n",
        "            # no_more_unknowns will return -1 if all pairs have been found\n",
        "            no_more_unknowns, worker_id_inner, _, unknown_prov_image, _, (\n",
        "                unknown_prov_filepath) = (next(generator_unknown_prov_images))\n",
        "\n",
        "            # Stop this inner loop if all anchor-unknown pairs for the current\n",
        "            # anchor image are found\n",
        "            if not(0 in workers_terminated_inner):\n",
        "                break\n",
        "\n",
        "            if no_more_unknowns.item() < 0:\n",
        "                np.put(workers_terminated_inner, worker_id_inner, 1)\n",
        "                if not(0 in workers_terminated_inner):\n",
        "                    break\n",
        "            else:\n",
        "                concatenated = torch.cat((anchor_image, unknown_prov_image), 0)\n",
        "\n",
        "                anchor_embedding, unknown_embedding = net(\n",
        "                    Variable(anchor_image).cuda(),\n",
        "                    Variable(unknown_prov_image).cuda())\n",
        "\n",
        "                euclidean_distance = F.pairwise_distance(\n",
        "                    anchor_embedding, unknown_embedding)\n",
        "\n",
        "                # Show the images:\n",
        "                imshow(\n",
        "                    torchvision.utils.make_grid(concatenated),\n",
        "                    'Dissimilarity: {:.2f}'.format(euclidean_distance.item()))\n",
        "\n",
        "                # Show the paths for the two images\n",
        "                print('Image 1: {}'.format(anchor_filepath[0]))\n",
        "                print('Image 2: {}'.format(unknown_prov_filepath[0]))\n",
        "                print('Dissimilarity: {:.2f}'.format(\n",
        "                    euclidean_distance.item()))\n",
        "\n",
        "print('Finshed all anchor-unknown pair comparisons')\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVt-evEFFP62",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## The End\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggsLeWXqNRij",
        "colab_type": "text"
      },
      "source": [
        "The two code blocks below print out the structure of the neural network, and the version info of all of the loaded packages in this environment. This information is useful for replicating this notebook in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDnSSRJpUuzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "model = net\n",
        "print(model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2x_rKjj7UxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# watermark is not installed by default.\n",
        "# the first time through, uncomment the two lines below\n",
        "# then run the block.\n",
        "#\n",
        "#\n",
        "# !pip install watermark\n",
        "# %load_ext watermark\n",
        "%watermark -v -m -p numpy, scipy, torchvision, PIL, tensorflow, torch -g\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}